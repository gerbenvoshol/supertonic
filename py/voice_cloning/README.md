# Voice Cloning Knowledge Distillation Pipeline

Transform any voice recording into a Supertonic-compatible style vector for instant voice cloning. This pipeline uses knowledge distillation to train a neural network encoder that inverts the Supertonic TTS process: **audio ‚Üí style vectors**.

## üéØ Overview

The Supertonic TTS system uses two style vectors to control voice characteristics:
- **`style_ttl`** [1, 50, 256] = 12,800 floats ‚Äî Controls voice timbre, tone, and prosody
- **`style_dp`** [1, 8, 16] = 128 floats ‚Äî Controls speaking rate and rhythm

This pipeline trains an encoder to extract these vectors from audio recordings, enabling:
- ‚úÖ **Free, on-device voice cloning** (no API calls)
- ‚úÖ **Custom voice creation** from any audio recording
- ‚úÖ **Fast inference** via ONNX Runtime
- ‚úÖ **Privacy-preserving** (all processing happens locally)

## üöÄ Quick Start

### 1. Install Dependencies

```bash
cd py/voice_cloning
pip install -r requirements.txt
```

### 2. Download Assets

The pipeline requires Supertonic TTS models and voice styles:

```bash
cd ../../c
./resource.sh  # Downloads to ../../assets/
cd ../py/voice_cloning
```

### 3. Generate Training Data

Create a synthetic dataset by running the TTS models with random style vectors:

```bash
# Generate 10,000 training samples + 1,000 validation + 500 test
python data_generator.py \
    --num-samples 10000 \
    --num-val-samples 1000 \
    --num-test-samples 500 \
    --output-dir ./training_data

# Takes ~2-4 hours on CPU, faster on GPU
```

### 4. Train the Encoder

Train a CNN encoder to predict style vectors from mel-spectrograms:

```bash
python train.py \
    --data-dir ./training_data \
    --checkpoint-dir ./checkpoints \
    --epochs 100 \
    --batch-size 32 \
    --lr 1e-4

# Takes ~4-8 hours on GPU (NVIDIA RTX 3090)
```

### 5. Export to ONNX

Convert the trained model to ONNX for production deployment:

```bash
python export_onnx.py \
    --checkpoint ./checkpoints/best_encoder.pth \
    --output ./output/audio_encoder.onnx \
    --optimize \
    --validate
```

### 6. Clone a Voice

Use your trained encoder to clone any voice:

```bash
# Clone from a WAV file
python clone_voice.py \
    --input my_voice_recording.wav \
    --encoder ./output/audio_encoder.onnx \
    --output my_voice_style.json

# Test the cloned voice
python clone_voice.py \
    --input my_voice_recording.wav \
    --encoder ./output/audio_encoder.onnx \
    --output my_voice_style.json \
    --test \
    --test-text "Hello, this is my cloned voice!"
```

### 7. Use with Supertonic TTS

```bash
# Python
cd ../
python example_onnx.py \
    --voice-style voice_cloning/my_voice_style.json \
    --text "Your custom text here"

# C
cd ../../c
./example_onnx \
    --voice-style ../py/voice_cloning/my_voice_style.json \
    --text "Your custom text here"
```

## üìÅ File Structure

```
py/voice_cloning/
‚îú‚îÄ‚îÄ README.md                    # This file
‚îú‚îÄ‚îÄ requirements.txt             # Python dependencies
‚îú‚îÄ‚îÄ config.py                    # Training configuration
‚îú‚îÄ‚îÄ utils.py                     # Shared utilities (audio, helpers)
‚îÇ
‚îú‚îÄ‚îÄ data_generator.py            # Phase 1: Generate synthetic training data
‚îú‚îÄ‚îÄ dataset.py                   # PyTorch Dataset with augmentation
‚îú‚îÄ‚îÄ encoder_model.py             # CNN and Transformer architectures
‚îÇ
‚îú‚îÄ‚îÄ train.py                     # Phase 2: Train the encoder
‚îú‚îÄ‚îÄ evaluate.py                  # Phase 3: Evaluate metrics
‚îú‚îÄ‚îÄ export_onnx.py               # Phase 4: Export to ONNX
‚îú‚îÄ‚îÄ clone_voice.py               # Phase 5: End-to-end voice cloning
‚îÇ
‚îî‚îÄ‚îÄ training_data/               # Generated by data_generator.py
    ‚îú‚îÄ‚îÄ train/                   # Training samples (.npz files)
    ‚îú‚îÄ‚îÄ val/                     # Validation samples
    ‚îú‚îÄ‚îÄ test/                    # Test samples
    ‚îî‚îÄ‚îÄ metadata.json            # Generation parameters
```

## üèóÔ∏è Architecture

### Knowledge Distillation Pipeline

```
Phase 1: Synthetic Data Generation (data_generator.py)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ for each sample:                                  ‚îÇ
‚îÇ   1. Sample random style (ttl + dp)              ‚îÇ
‚îÇ   2. Pick random text                             ‚îÇ
‚îÇ   3. TTS synthesize: style ‚Üí audio                ‚îÇ
‚îÇ   4. Save triplet: (audio, style_ttl, style_dp)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Phase 2: Encoder Training (train.py)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ encoder = AudioEncoder()                          ‚îÇ
‚îÇ for each (audio, true_ttl, true_dp):            ‚îÇ
‚îÇ   1. mel = mel_spectrogram(audio)                ‚îÇ
‚îÇ   2. pred_ttl, pred_dp = encoder(mel)            ‚îÇ
‚îÇ   3. loss = MSE(pred, true) + cosine_sim         ‚îÇ
‚îÇ   4. loss.backward()                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Phase 3: Deployment (export_onnx.py + clone_voice.py)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. Export encoder to ONNX                         ‚îÇ
‚îÇ 2. Load audio ‚Üí extract mel ‚Üí run encoder        ‚îÇ
‚îÇ 3. Get style vectors ‚Üí save as JSON               ‚îÇ
‚îÇ 4. Use with Supertonic TTS                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### CNN Encoder Architecture (Default)

```
Input: Mel-spectrogram [B, 80, T]
‚Üì
Conv Block 1: Conv1d(80‚Üí128, k=7, s=2) + BatchNorm + GELU
Conv Block 2: Conv1d(128‚Üí256, k=5, s=2) + BatchNorm + GELU
Conv Block 3: Conv1d(256‚Üí512, k=3, s=2) + BatchNorm + GELU
Conv Block 4: Conv1d(512‚Üí512, k=3, s=2) + BatchNorm + GELU
Conv Block 5: Conv1d(512‚Üí512, k=3, s=1) + BatchNorm + GELU
‚Üì
Attention Pooling (learns to attend to important frames)
‚Üì
FC(512‚Üí512) + GELU + Dropout
‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ style_ttl head      ‚îÇ style_dp head       ‚îÇ
‚îÇ FC(512‚Üí12800)       ‚îÇ FC(512‚Üí128)         ‚îÇ
‚îÇ reshape [1,50,256]  ‚îÇ reshape [1,8,16]    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Parameters: ~5.2M
```

## üìä Configuration

Edit `config.py` or pass command-line arguments to customize:

### Data Generation
- `num_samples`: Number of training samples (default: 10,000)
- `num_val_samples`: Validation samples (default: 1,000)
- `style_sample_mode`: `random`, `perturb`, `interpolate`, or `mixed`
- `texts`: Diverse text corpus for synthesis
- `languages`: Languages to generate (`en`, `ko`, `es`, `pt`, `fr`)

### Model Architecture
- `encoder_type`: `cnn` (fast, ~5M params) or `transformer` (powerful, ~12M params)
- `hidden_dim`: Hidden dimension (default: 512)
- `num_layers`: Number of layers (default: 6)
- `dropout`: Dropout rate (default: 0.1)

### Training
- `batch_size`: Batch size (default: 32)
- `learning_rate`: Initial learning rate (default: 1e-4)
- `epochs`: Number of epochs (default: 100)
- `use_amp`: Mixed precision training (default: True)
- `lambda_ttl`, `lambda_dp`, `lambda_cosine`: Loss weights

### Audio Processing
- `sample_rate`: 24000 Hz (must match Supertonic TTS)
- `n_mels`: 80 mel filterbanks
- `n_fft`: 1024
- `hop_length`: 256

### Data Augmentation
- Noise injection (SNR 20-40 dB)
- Time stretching (0.9-1.1x)
- Pitch shifting (¬±1 semitone)
- Volume scaling (0.8-1.2x)
- SpecAugment (time/frequency masking)

## üí° Usage Examples

### Example 1: Quick Training with Small Dataset

```bash
# Generate small dataset for testing
python data_generator.py \
    --num-samples 1000 \
    --num-val-samples 100 \
    --num-test-samples 50 \
    --output-dir ./quick_test

# Train for a few epochs
python train.py \
    --data-dir ./quick_test \
    --epochs 10 \
    --batch-size 16
```

### Example 2: Production Training with Large Dataset

```bash
# Generate large dataset
python data_generator.py \
    --num-samples 50000 \
    --num-val-samples 5000 \
    --num-test-samples 2000 \
    --style-sample-mode mixed \
    --output-dir ./production_data

# Train with full configuration
python train.py \
    --data-dir ./production_data \
    --checkpoint-dir ./checkpoints \
    --epochs 100 \
    --batch-size 64 \
    --lr 1e-4 \
    --device cuda

# Evaluate
python evaluate.py \
    --checkpoint ./checkpoints/best_encoder.pth \
    --data-dir ./production_data \
    --output-dir ./evaluation_results

# Export
python export_onnx.py \
    --checkpoint ./checkpoints/best_encoder.pth \
    --output ./models/audio_encoder.onnx \
    --optimize
```

### Example 3: Clone Multiple Voices

```bash
# Clone a batch of voices
for audio in voice_samples/*.wav; do
    basename=$(basename "$audio" .wav)
    python clone_voice.py \
        --input "$audio" \
        --encoder ./models/audio_encoder.onnx \
        --output "cloned_styles/${basename}_style.json" \
        --name "${basename}" \
        --test \
        --test-output "cloned_samples/${basename}_test.wav"
done
```

### Example 4: Use Custom Configuration

```yaml
# config.yaml
num_samples: 20000
encoder_type: transformer
hidden_dim: 768
num_layers: 12
batch_size: 32
learning_rate: 5e-5
use_amp: true
```

```bash
python train.py --config config.yaml
```

## üìà Hardware Requirements

### Minimum Requirements
- **CPU**: 4 cores (data generation + inference)
- **RAM**: 16 GB
- **Storage**: 50 GB for dataset + models
- **GPU**: Optional but highly recommended for training

### Recommended for Training
- **CPU**: 8+ cores (Intel i7/i9, AMD Ryzen 7/9)
- **RAM**: 32 GB
- **GPU**: NVIDIA RTX 3090 or better (24 GB VRAM)
- **Storage**: 100 GB SSD

### Performance Estimates
| Task | CPU (16-core) | GPU (RTX 3090) |
|------|---------------|----------------|
| Data Generation (10k samples) | ~3-4 hours | ~1-2 hours |
| Training (100 epochs, 10k samples) | ~24-48 hours | ~4-8 hours |
| ONNX Inference (per sample) | ~50-100 ms | ~5-10 ms |

## üé® Expected Quality

### Training Metrics (After 100 Epochs)
- **MSE (style_ttl)**: ~0.02-0.05
- **MSE (style_dp)**: ~0.01-0.03
- **Cosine Similarity (ttl)**: >0.95
- **Cosine Similarity (dp)**: >0.98

### Voice Cloning Quality
The encoder learns to capture:
- ‚úÖ **Voice timbre** (warm/bright/dark)
- ‚úÖ **Pitch range** (high/low voice)
- ‚úÖ **Speaking rate** (fast/slow)
- ‚úÖ **Prosody patterns** (intonation, stress)

**Note**: Quality depends on:
1. Input audio quality (clear, 16kHz+, minimal background noise)
2. Recording length (5-30 seconds recommended)
3. Training dataset size (more data = better generalization)
4. Model architecture (transformer > CNN for quality)

## üîß Troubleshooting

### Issue: "Out of memory during training"

**Solution**: Reduce batch size or use gradient accumulation

```bash
python train.py --batch-size 16  # Instead of 32
```

### Issue: "Training loss not decreasing"

**Solutions**:
1. Check data quality: `python data_generator.py --num-samples 100`
2. Increase learning rate: `--lr 5e-4`
3. Try different architecture: `--config config.yaml` (set `encoder_type: transformer`)
4. Increase dataset size: `--num-samples 50000`

### Issue: "Cloned voice doesn't sound right"

**Solutions**:
1. Use higher quality input audio (clean, 24kHz, 10-30 seconds)
2. Train on more diverse data (multilingual, multiple speakers)
3. Fine-tune on specific voice styles
4. Use transformer encoder for better quality

### Issue: "Data generation is slow"

**Solutions**:
1. Use GPU if available: `--use-gpu`
2. Generate smaller dataset first: `--num-samples 1000`
3. Run on multiple machines and combine datasets
4. Use faster TTS settings: `--tts-total-step 3`

### Issue: "ONNX export fails"

**Solutions**:
1. Check PyTorch and ONNX versions: `pip install --upgrade torch onnx onnxruntime`
2. Use CPU for export: `--device cpu`
3. Try different opset version: `--opset 13`

## üìö Additional Resources

### Training Tips
1. **Start small**: Test the pipeline with 1000 samples before generating a large dataset
2. **Monitor training**: Use TensorBoard to track losses and metrics
3. **Save checkpoints**: Training can take hours; save frequently
4. **Validate often**: Check validation loss to avoid overfitting
5. **Use mixed precision**: Enable `use_amp=True` for faster training with less memory

### Data Generation Tips
1. **Diverse texts**: Use varied sentence structures, lengths, and phonemes
2. **Style sampling**: Use `mixed` mode for the most diverse dataset
3. **Language mixing**: Train on multiple languages for better generalization
4. **Batch generation**: Generate data incrementally and combine datasets

### Voice Cloning Tips
1. **Audio quality**: Use clean, high-quality recordings (16kHz+, mono)
2. **Recording length**: 5-30 seconds is optimal (too short = unstable, too long = averaged)
3. **Content**: Natural speech with varied intonation works best
4. **Testing**: Always test the cloned voice before production use

## üî¨ Advanced Usage

### Custom Loss Functions

Edit `train.py` to add custom losses:

```python
# Example: Add spectral loss
class CustomMultiLoss(nn.Module):
    def forward(self, pred_ttl, true_ttl, pred_dp, true_dp, pred_mel, true_mel):
        loss_ttl = F.mse_loss(pred_ttl, true_ttl)
        loss_dp = F.mse_loss(pred_dp, true_dp)
        loss_spectral = spectral_convergence(pred_mel, true_mel)
        return loss_ttl + loss_dp + 0.1 * loss_spectral
```

### Fine-Tuning on Specific Voices

```bash
# Train base model
python train.py --data-dir ./base_data --checkpoint-dir ./base_checkpoints

# Fine-tune on specific voice
python train.py \
    --data-dir ./specific_voice_data \
    --checkpoint-dir ./finetuned_checkpoints \
    --resume ./base_checkpoints/best_encoder.pth \
    --epochs 20 \
    --lr 1e-5  # Lower learning rate for fine-tuning
```

### Distributed Training

```bash
# Multi-GPU training (coming soon)
python -m torch.distributed.launch \
    --nproc_per_node=4 \
    train.py --data-dir ./training_data
```

## üìù Citation

If you use this voice cloning pipeline in your research or application, please cite:

```bibtex
@software{supertonic_voice_cloning,
  title = {Supertonic Voice Cloning Knowledge Distillation Pipeline},
  author = {Supertone Inc.},
  year = {2026},
  url = {https://github.com/supertone-inc/supertonic}
}
```

## üìú License

This voice cloning pipeline is part of the Supertonic project and is licensed under the same terms as the main repository. See the main [LICENSE](../../LICENSE) file for details.

## ü§ù Contributing

Contributions are welcome! Please:
1. Open an issue to discuss major changes
2. Follow the existing code style
3. Add tests for new features
4. Update documentation

## üìß Support

- **Issues**: https://github.com/supertone-inc/supertonic/issues
- **Discussions**: https://github.com/supertone-inc/supertonic/discussions
- **Documentation**: https://supertone-inc.github.io/supertonic-py

---

**Note**: This is an experimental feature for research and educational purposes. For production-quality voice cloning, consider using the official [Voice Builder](https://supertonic.supertone.ai/voice_builder) service.
